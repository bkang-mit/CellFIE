{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233e3e3c",
   "metadata": {},
   "source": [
    "## 080321\n",
    "## This notebook contains functions useful for analyzing Omics Integrator Outputs\n",
    "## Cell specific function and general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8711e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gprofiler import GProfiler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import OmicsIntegrator as oi\n",
    "import community\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77fe0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca55000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfs/latdata/bkang/Omics Integrator Pipeline'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb49d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Arial\"\n",
    "matplotlib.rcParams['font.family'] = \"Arial\"\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68c6e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Naive': <networkx.classes.graph.Graph at 0x7f26635075f8>,\n",
       " '1_Astrocyte': <networkx.classes.graph.Graph at 0x7f24cf7d8d68>,\n",
       " '1_GABAergic': <networkx.classes.graph.Graph at 0x7f24cf6fc7b8>,\n",
       " '1_Microglia': <networkx.classes.graph.Graph at 0x7f24cf6985c0>,\n",
       " '1_OPC': <networkx.classes.graph.Graph at 0x7f24cf61cbe0>,\n",
       " '1_Oligodendrocyte': <networkx.classes.graph.Graph at 0x7f24cf5b1e48>,\n",
       " '3_Astrocyte': <networkx.classes.graph.Graph at 0x7f24cf4c4eb8>,\n",
       " '3_GABAergic': <networkx.classes.graph.Graph at 0x7f24cf400240>,\n",
       " '3_Microglia': <networkx.classes.graph.Graph at 0x7f24cf394278>,\n",
       " '3_OPC': <networkx.classes.graph.Graph at 0x7f24cf30ccc0>,\n",
       " '3_Oligodendrocyte': <networkx.classes.graph.Graph at 0x7f24cf29afd0>,\n",
       " '5_Astrocyte': <networkx.classes.graph.Graph at 0x7f24cf1c7470>,\n",
       " '5_GABAergic': <networkx.classes.graph.Graph at 0x7f24cf157ba8>,\n",
       " '5_Microglia': <networkx.classes.graph.Graph at 0x7f24cf0f7860>,\n",
       " '5_OPC': <networkx.classes.graph.Graph at 0x7f24cf0086d8>,\n",
       " '5_Oligodendrocyte': <networkx.classes.graph.Graph at 0x7f24cefaa278>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynetworks = pickle.load(open(\"./OI_Results/pickles/080421_networks/mynetworks.pkl\", \"rb\"))\n",
    "mynetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = mynetworks['Naive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4ed9b",
   "metadata": {},
   "source": [
    "## Useful functions for analyzing randomization output from OI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9761ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# functions to use\n",
    "def get_avg_cost(graph):\n",
    "    df = nx.to_pandas_edgelist(graph)\n",
    "    return sum(df['cost'])/len(df)\n",
    "\n",
    "def jaccard(L1, L2):\n",
    "    for i in range(len(L1)):\n",
    "        for j in range(len(L2)):\n",
    "            if (L1[i][0] == L2[j][1]) & (L1[i][1] == L2[j][0]):\n",
    "                L2[j] = (L2[j][1], L2[j][0])\n",
    "            \n",
    "    intersection = len(list(set(L1).intersection(L2)))\n",
    "    #print(intersection)\n",
    "    union = (len(L1) + len(L2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "# use this to convert greene et al network into gene symbols\n",
    "def convert_entrez_to_symbols(df_input):\n",
    "    df = df_input.copy()\n",
    "    gene_symbols = {}\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    genes = df[0].values.tolist()\n",
    "    annotations = mg.getgenes(genes, fields='symbol,entrezgene')\n",
    "    #print(annotations)\n",
    "    for gene in annotations:\n",
    "        if 'symbol' in gene.keys():\n",
    "            gene_symbols[gene['query']] = gene['symbol']\n",
    "        else:\n",
    "            gene_symbols[gene['query']] = gene['query']\n",
    "    \n",
    "    symbol_list_to_add = []\n",
    "    for gene in genes:\n",
    "        symbol_list_to_add.append(gene_symbols[gene])\n",
    "    df[0] = symbol_list_to_add\n",
    "    \n",
    "    gene_symbols = {}\n",
    "    mg = mygene.MyGeneInfo()\n",
    "    genes = df[1].values.tolist()\n",
    "    annotations = mg.getgenes(genes, fields='symbol,entrezgene')\n",
    "    #print(annotations)\n",
    "    for gene in annotations:\n",
    "        if 'symbol' in gene.keys():\n",
    "            gene_symbols[gene['query']] = gene['symbol']\n",
    "        else:\n",
    "            gene_symbols[gene['query']] = gene['query']\n",
    "            \n",
    "    symbol_list_to_add = []\n",
    "    for gene in genes:\n",
    "        symbol_list_to_add.append(gene_symbols[gene])\n",
    "    df[1] = symbol_list_to_add\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_basic_statistics(graph,robust_results): # graph.generate_basic_statistics\n",
    "    robust_summary = {}\n",
    "\n",
    "    for paramstring, robust_network in robust_results.items(): \n",
    "        \n",
    "        if robust_network.number_of_nodes() != 0: \n",
    "                \n",
    "            robust_df = oi.get_networkx_graph_as_dataframe_of_nodes(robust_network)\n",
    "            \n",
    "            robust_edges_df = nx.to_pandas_edgelist(robust_network)\n",
    "           # sum_cost = \n",
    "            robust_summary[paramstring] = {\n",
    "                \"W\":                paramstring.split(\"_\")[1],\n",
    "                \"B\":                paramstring.split(\"_\")[3],\n",
    "                \"G\":                paramstring.split(\"_\")[5],\n",
    "                \"K\":                paramstring.split(\"_\")[7],\n",
    "                \"size\":             len(robust_df), \n",
    "                \"min_robustness\":   robust_df.robustness.min(), \n",
    "                \"mean_robustness\":  robust_df.robustness.mean(), \n",
    "                \"max_specificity\":  robust_df.specificity.max(),\n",
    "                \"mean_specificity\": robust_df.specificity.mean(), \n",
    "                \"mean_log_degree\":  np.log2(robust_df.degree).mean(), \n",
    "                \"std_log_degree\":   np.log2(robust_df.degree).std(), \n",
    "                \"KS_statistic\":     stats.ks_2samp(np.log2(robust_df.degree), np.log2(graph.node_attributes.degree))[0],\n",
    "                \"num_fly_prize\": len(robust_df[robust_df.source == 'fly genetic screen']),\n",
    "                \"num_HD_prize\" : len(robust_df[robust_df.source == \"huntington's AOO modifiers\"]),\n",
    "                \"mean_edge_cost\": get_avg_cost(robust_network)\n",
    "                \n",
    "                                     \n",
    "                }\n",
    "                \n",
    "    robust_summary = pd.DataFrame.from_dict(robust_summary, orient='index')\n",
    "    return robust_summary\n",
    "\n",
    "def filter_by_max_specificity(robust_summary, max_specificity):\n",
    "    # initial filter by max-specificity > 0.9\n",
    "    robust_summary = robust_summary[robust_summary['max_specificity'] >= 0.90]\n",
    "    \n",
    "    node_attributes_df = oi.get_networkx_graph_as_dataframe_of_nodes(nxgraph)\n",
    "    node_attributes_df = node_attributes_df[node_attributes_df[\"robustness\"] > min_robustness]\n",
    "    node_attributes_df = node_attributes_df[node_attributes_df[\"specificity\"] < max_specificity]\n",
    "    robust_network = nxgraph.subgraph(node_attributes_df.index[:]).copy()\n",
    "    return robust_network\n",
    "\n",
    "def get_filtered_subgraph_from_randomizations(nxgraph, min_robustness, max_specificity):\n",
    "\n",
    "    if nxgraph.number_of_nodes() ==0:\n",
    "        oi.logger.warning(\"Augmented forest is empty.\")\n",
    "        return nxgraph\n",
    "\n",
    "    node_attributes_df = oi.get_networkx_graph_as_dataframe_of_nodes(nxgraph)\n",
    "    node_attributes_df = node_attributes_df[node_attributes_df[\"robustness\"] > min_robustness]\n",
    "    node_attributes_df = node_attributes_df[node_attributes_df[\"specificity\"] < max_specificity]\n",
    "    robust_network = nxgraph.subgraph(node_attributes_df.index[:]).copy()\n",
    "    return robust_network\n",
    "\n",
    "# Returns a dictionary containing subgraph copies of each louvain cluster\n",
    "# in original dataframe, cluster is in string\n",
    "def get_subgraph_from_clusters(nxgraph, cluster_type):\n",
    "    clusters = {}\n",
    "    if nxgraph.number_of_nodes()==0:\n",
    "        oi.logger.warning(\"Augmented forest is empty.\")\n",
    "        return nxgraph\n",
    "    node_attributes_df = oi.get_networkx_graph_as_dataframe_of_nodes(nxgraph)\n",
    "    temp_list = node_attributes_df.loc[:,cluster_type].tolist()\n",
    "    mylist = [int(i) for i in temp_list]\n",
    "    for cluster in range(max(mylist)+1):\n",
    "        subset = node_attributes_df[node_attributes_df[cluster_type] == str(cluster)]\n",
    "        clusters[cluster] = nxgraph.subgraph(subset.index[:]).copy()\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# lv_clusters can be replaced with greedy or leiden cluster\n",
    "# lv_clusters param is a dictionary containing graphs\n",
    "def get_pathway_enrichment(sourcelist, lv_clusters, prec_thresh, int_size_thresh):\n",
    "    gp = GProfiler(return_dataframe=True)\n",
    "    lv_clusters_df = {cluster: oi.get_networkx_graph_as_dataframe_of_nodes(network) for cluster, network in lv_clusters.items()}\n",
    "    Qdict = {cluster: df.index.values.tolist() for cluster, df in lv_clusters_df.items()}\n",
    "    enrichment_results = {cluster: gp.profile(organism='hsapiens',query=qlist, sources=sourcelist, no_evidences = False) for cluster, qlist in Qdict.items()}\n",
    "\n",
    "    enrichment_results = {cluster: df[(df['precision'] >= prec_thresh) & (df['intersection_size'] >= int_size_thresh)] for cluster, df in enrichment_results.items()}\n",
    "    \n",
    "    return enrichment_results\n",
    "\n",
    "def annotate_pathway(network, enrichment_result, loc_list):\n",
    "    # enrichment_result is dataframe (for each cluster)\n",
    "    for i in range(len(loc_list)):\n",
    "        loc = loc_list[i]\n",
    "        nodes = enrichment_result.iloc[loc]['intersections']\n",
    "        pval = enrichment_result.iloc[loc]['p_value']\n",
    "        source = enrichment_result.iloc[loc]['native']\n",
    "        name = enrichment_result.iloc[loc]['name']\n",
    "\n",
    "        for node in nodes:\n",
    "            pathname = 'path_' +str(i+1)\n",
    "            pvalname = 'pval_' +str(i+1)\n",
    "            nx.set_node_attributes(network, {node: {pathname: source + ': ' + name}})\n",
    "            nx.set_node_attributes(network, {node: {pvalname: str(pval)}})\n",
    "    \n",
    "def annotate_pathway_overlap(network, enrichment_result, loc_list):\n",
    "    # enrichment_result is dataframe (for each cluster)\n",
    "    loc_list.reverse()\n",
    "    for i in range(len(loc_list)):\n",
    "        loc = loc_list[i]\n",
    "        nodes = enrichment_result.iloc[loc]['intersections']\n",
    "        pval = enrichment_result.iloc[loc]['p_value']\n",
    "        source = enrichment_result.iloc[loc]['native']\n",
    "        name = enrichment_result.iloc[loc]['name']\n",
    "\n",
    "        for node in nodes:\n",
    "            nx.set_node_attributes(network, {node: {'pathway': source + ': ' + name}})\n",
    "            \n",
    "            \n",
    "def invert(list_of_lists): return {item: i for i, list in enumerate(list_of_lists) for item in list}\n",
    "\n",
    "def greedy_clustering(nxgraph):\n",
    "    \n",
    "    clustering = pd.Series(invert(nx.algorithms.community.greedy_modularity_communities(nxgraph)), \n",
    "                           name='greedy_clusters').astype(str).reindex(nxgraph.nodes())\n",
    "    nx.set_node_attributes(nxgraph, clustering.to_frame().to_dict(orient='index'))\n",
    "    \n",
    "def leiden_clustering(nxgraph):\n",
    "    \n",
    "    clustering = pd.Series(invert(algorithms.leiden(nxgraph).communities), name='leiden_clusters').astype(str).reindex(nxgraph.nodes())\n",
    "    nx.set_node_attributes(nxgraph, clustering.to_frame().to_dict(orient='index'))\n",
    "    \n",
    "def louvain_clustering(nxgraph,resolution, random_state):\n",
    "    nx.set_node_attributes(nxgraph, {node: {'louvain_clusters':str(cluster)} for node,cluster in community.best_partition(nxgraph, resolution=resolution, \n",
    "                                                                                                                          random_state=random_state).items()})\n",
    "    \n",
    "def sort_graph(nxgraph):\n",
    "    nodes_sorted = sorted(nxgraph.nodes(data=True), key= lambda x: (x[1]['source']))\n",
    "    edges = sorted(nxgraph.edges(data=True))\n",
    "    sorted_graph = nx.Graph()\n",
    "    sorted_graph.add_nodes_from(nodes_sorted)\n",
    "    sorted_graph.add_edges_from(edges)\n",
    "    return sorted_graph\n",
    "\n",
    "def get_avg_degree(nxgraph):\n",
    "    s = 0\n",
    "    for node in nxgraph.nodes():\n",
    "        s += nxgraph.degree(node)\n",
    "    return s/nxgraph.number_of_nodes()\n",
    "\n",
    "def get_network_stats(clusters):\n",
    "    num_nodes = []\n",
    "    avg_degree = []\n",
    "    \n",
    "    for cluster, network in clusters.items():\n",
    "        print(cluster, network.number_of_nodes(), get_avg_degree(network))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d281843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robust_subgraph_from_randomizations(nxgraph, max_size=400, min_component_size=5, min_robustness=0):\n",
    "    \"\"\"\n",
    "    Given a graph with robustness attributes, take the top `max_size` robust nodes and\n",
    "    prune any \"small\" components.\n",
    "\n",
    "    Arguments:\n",
    "        nxgraph (networkx.Graph): Network from randomization experiment\n",
    "        max_size (int): Max size of robust network\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Robust network\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Potential alternative approach - from entire network, attempt to remove lowest robustness node.\n",
    "    # If removal results in a component of size less than min_size, do not remove.\n",
    "\n",
    "    if nxgraph.number_of_nodes() == 0:\n",
    "   #     logger.warning(\"Augmented forest is empty.\")\n",
    "        return nxgraph\n",
    "\n",
    "    # Get indices of top nodes sorted by high robustness, then low specificity. Don't allow nodes with robustness = 0.\n",
    "    node_attributes_df = oi.get_networkx_graph_as_dataframe_of_nodes(nxgraph)\n",
    "    node_attributes_df = node_attributes_df[node_attributes_df[\"robustness\"] > min_robustness]\n",
    "    node_attributes_df.sort_values([\"robustness\", \"specificity\"], ascending=[False, True], inplace=True)\n",
    "\n",
    "    # Find the largest subgraph of `nxgraph` such that the number of nodes, after filtering out small components, is \n",
    "    # not more than `max_size`. This is accomplished by progressively loosening the threshold for inclusion in the subgraph. \n",
    "    for top_n in np.arange(min(max_size, len(node_attributes_df)), len(node_attributes_df)+1): \n",
    "        # Get robust subnetwork and remove small components.\n",
    "        robust_network = nxgraph.subgraph(node_attributes_df.index[:top_n])\n",
    "        robust_network = filter_graph_by_component_size(robust_network, min_component_size)\n",
    "\n",
    "        # The number of nodes in robust network is monotonically increasing with `top_n`, so once a robust network is found\n",
    "        # such that the number of nodes exceeds `max_size`, we do not need to iterate through the rest of the for loop and\n",
    "        # can return the last valid robust network. \n",
    "        if robust_network.number_of_nodes() <= max_size: \n",
    "            robust_network_out = robust_network\n",
    "        else: \n",
    "            return robust_network_out\n",
    "\n",
    "    return robust_network_out\n",
    "\n",
    "def filter_graph_by_component_size(nxgraph, min_size=5):\n",
    "    \"\"\"\n",
    "    Removes any components that are less than `min_size`.\n",
    "\n",
    "    Arguments:\n",
    "        nxgraph (networkx.Graph): Network from randomization experiment\n",
    "        min_size (int): Min size of components in `nxgraph`. Set to 2 to remove singletons only.\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Network with components less than specified size removed.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_subgraph = nxgraph.copy()\n",
    "\n",
    "    small_components = [nxgraph.subgraph(c).nodes() for c in nx.connected_components(nxgraph) if nxgraph.subgraph(c).number_of_nodes() < min_size]\n",
    "    filtered_subgraph.remove_nodes_from(oi.flatten(small_components))\n",
    "\n",
    "    return filtered_subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91a0eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_networkx_graph_as_interactive_html(nxgraph, attribute_metadata=dict(), output_dir=\".\", filename=\"graph.html\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        nxgraph (networkx.Graph): any instance of networkx.Graph\n",
    "        output_dir (str): the directory in which to output the file\n",
    "        filename (str): the filename of the output file\n",
    "    Returns:\n",
    "        Path: the filepath which was outputted to\n",
    "    \"\"\"\n",
    "    return axial.graph(nxgraph,\n",
    "        title='OmicsIntegrator2 Results',\n",
    "        scripts_mode=\"inline\",\n",
    "        data_mode=\"inline\",\n",
    "        output_dir=output_dir,\n",
    "        filename=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe3e1b",
   "metadata": {},
   "source": [
    "## Cell Specific Network Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602dd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_df is interactome dataframe with cell_rank columns\n",
    "# celltype: string (e.g. \"Astrocyte_rank\")\n",
    "# network: your query network\n",
    "\n",
    "# pilot approach: use the same interactome annotations (based on edges, taking min rank (less expresed)\n",
    "def get_average_rank_score_from_interactome_dataframe(celltype, int_df, network):\n",
    "    # get edge list\n",
    "    edges_df = nx.to_pandas_edgelist(network)\n",
    "    scores = 0\n",
    "    for index, rows in edges_df.iterrows():\n",
    "        protein1 = rows['source']\n",
    "        protein2 = rows['target']\n",
    "        scores_to_add = int_df[(int_df['protein1'] == protein1) & (int_df['protein2'] == protein2)][celltype]\n",
    "        scores = scores + scores_to_add\n",
    "    # average\n",
    "    final_score = scores/len(robust_edges_df)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6714dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# networks: dictionary of networks\n",
    "def jaccard_network(networks):\n",
    "    pairwise_jaccard_df = pd.DataFrame(index = list(networks.keys()), columns = list(networks.keys()))\n",
    "    for key1 in pairwise_jaccard_df.index:\n",
    "        for key2 in pairwise_jaccard_df.index:\n",
    "            network_1 = networks[key1]\n",
    "            network_2 = networks[key2]\n",
    "            L1 = list(network_1.edges)\n",
    "            L2 = list(network_2.edges)\n",
    "            J = jaccard(L1, L2)\n",
    "            pairwise_jaccard_df.loc[key1, key2] = J\n",
    "            pairwise_jaccard_df = pairwise_jaccard_df.astype('float')\n",
    "    return pairwise_jaccard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c99d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function implements GSEA running ES score calculation\n",
    "# Input:\n",
    "# ranked_gene_list: a list of gene names ranked by its phenotype_corr_vals\n",
    "# phenotype_corr_vals: a list of phenotype correlation values used to rank genes, \n",
    "#                         should be in the same order as ranked_gene_list \n",
    "# gene_set : user defined gene set (a list of gene names)\n",
    "# p : P_hit exponent value\n",
    "\n",
    "# ranked_genes_df : dataframe containing ranked genes and phenotype correlation values (i.e. logfoldchange here)\n",
    "\n",
    "# Output:\n",
    "# ES_vec: containes ES score for each gene, cumulative.\n",
    "# max_Es : ES score (positive or negative) that has absolute maximum value\n",
    "def GSEA(ranked_genes_df, gene_set, p):\n",
    "    \n",
    "    ranked_genes_df = ranked_genes_df.sort_values(by = 'logfoldchanges', ascending=False)\n",
    "    phenotype_corr_vals = ranked_genes_df['logfoldchanges'].values\n",
    "    ranked_gene_list = ranked_genes_df['names'].values\n",
    "    \n",
    "    \n",
    "    # initialize ES score, ES_vec\n",
    "    ES_score = 0\n",
    "    ES_vec = []\n",
    "    \n",
    "    # define N, N_H, N_R\n",
    "    N = len(ranked_gene_list)\n",
    "    N_H = len(gene_set)\n",
    "    \n",
    "    N_R = 0 # initialize N_R\n",
    "    for index, row in ranked_genes_df.iterrows():\n",
    "        if row['names'] in gene_set:\n",
    "            N_R += abs(row['logfoldchanges'])**p\n",
    "    \n",
    "    # walk down the ranked_gene_list\n",
    "    for i in range(len(ranked_gene_list)):\n",
    "        gene = ranked_gene_list[i]\n",
    "        if gene in gene_set:\n",
    "            # if gene is found in gene_set, update ES_score as follows\n",
    "            P_hit = (abs(phenotype_corr_vals[i]))**p/N_R\n",
    "            ES_score += P_hit\n",
    "            ES_vec.append(ES_score)\n",
    "        else:\n",
    "            # if not found, update ES_score as follows\n",
    "            P_miss = 1/(N-N_H)\n",
    "            ES_score -= P_miss\n",
    "            ES_vec.append(ES_score)\n",
    "    \n",
    "    # get maximum ES score (in both positive and negative)\n",
    "    max_ES = np.max(np.abs(ES_vec)) # find absolute maximum\n",
    "    for i in range(len(ES_vec)):\n",
    "        if abs(ES_vec[i]) == max_ES:\n",
    "            max_ES = ES_vec[i] # convert back to non-absolute value\n",
    "    return ES_vec, max_ES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Defining a function to use to permute labels\n",
    "# input: \n",
    "# ranked_genes:  dataframe containing ranked genes and absolute SNR values\n",
    "#                   to be permuted\n",
    "\n",
    "# output:\n",
    "# permuted_ranked_genes : data with gene labels permuted\n",
    "\n",
    "def permute_labels(ranked_genes_df):\n",
    "    \n",
    "    permuted_ranked_genes = ranked_genes_df.copy()\n",
    "    idx = np.random.permutation(len(ranked_genes_df))\n",
    "    permuted_ranked_genes['names'] = ranked_genes_df.iloc[idx]['names'].values\n",
    "    return permuted_ranked_genes\n",
    "    \n",
    "def permute_gene_sets(ranked_genes_df, gene_set):\n",
    "    import random\n",
    "    k = len(gene_set)\n",
    "    gene_list = list(ranked_genes_df['names'].values)\n",
    "    permuted_gene_set = random.sample(gene_list, k)\n",
    "    return permuted_gene_set \n",
    "    \n",
    "def get_null_enrichment_score(N, ranked_genes_df, gene_set, p):\n",
    "    # define other inputs\n",
    "    ranked_gene_list = ranked_genes_df['names'].values.tolist()\n",
    "    # p = 1\n",
    "    \n",
    "    # initialize ES_null\n",
    "    ES_null = []\n",
    "    print('permuting gene sets')\n",
    "    # for each permutation, update ES_null\n",
    "    for j in range(N): # N = 1000\n",
    "        # permute labels\n",
    "        #permuted_ranked_genes_df = permute_labels(ranked_genes_df)\n",
    "        \n",
    "        # permute gene set \n",
    "        permuted_gene_set = permute_gene_sets(ranked_genes_df, gene_set)\n",
    "        \n",
    "        # run GSEA with updated list and append to ES_null[i]\n",
    "        _, maxES_null = GSEA(ranked_genes_df, permuted_gene_set, p)\n",
    "        ES_null.append(maxES_null)\n",
    "    return ES_null\n",
    "\n",
    "# ES_null = list of ES values from random permutation\n",
    "# ES_true = query max ES to see whether it is as extreme as random distribution\n",
    "def get_empirical_pval(ES_null, ES_true):\n",
    "    if ES_true < 0: # if negative,\n",
    "        # subset negative portion\n",
    "        ES_null_subset= [x for x in ES_null if x < 0]\n",
    "        # total number of permutations to compare\n",
    "        N = len(ES_null_subset)\n",
    "        # update nominal p-val, left tail \n",
    "        if N==0:\n",
    "            pval = 0.0\n",
    "        else:\n",
    "            pval = len([x for x in ES_null_subset if x <= ES_true])/N\n",
    "    else:\n",
    "        # subset positive portion\n",
    "        ES_null_subset= [x for x in ES_null if x >= 0]\n",
    "        # total number of permutations to compare \n",
    "        N = len(ES_null_subset)\n",
    "        if N==0:\n",
    "            pval = 0.0\n",
    "        else:\n",
    "            # update nominal p-val, right tail\n",
    "            pval =len([x for x in ES_null_subset if x >= ES_true])/N\n",
    "\n",
    "    return pval\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e4100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6559928",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_df_1 = pd.DataFrame(index = list(final_networks_1.keys()), columns = list(final_networks_1.keys()))\n",
    "for key1 in pairwise_df_1.index:\n",
    "    for key2 in pairwise_df_1.index:\n",
    "        network_1 = final_networks_1[key1]\n",
    "        network_2 = final_networks_1[key2]\n",
    "        L1 = list(network_1.edges)\n",
    "        L2 = list(network_2.edges)\n",
    "        J = util.jaccard(L1, L2)\n",
    "        pairwise_df_1.loc[key1, key2] = J\n",
    "pairwise_df_1 = pairwise_df_1.astype('float')\n",
    "pairwise_df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f5454",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b9a426",
   "metadata": {},
   "source": [
    "## Annotate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d6957e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_graph(clusters):\n",
    "    anno_df = pd.read_csv('./effect_size_annotation.csv', index_col=0)\n",
    "    # annotate GWA effect size, direction\n",
    "    for index, row in anno_df.iterrows():\n",
    "        for i in range(0,len(clusters)):\n",
    "            network = clusters[i]\n",
    "            score=round(row['GWA Effect Size'], 2)\n",
    "            direction = row['Direction']\n",
    "            nx.set_node_attributes(network, {index: {'GWA Effect Size': score, 'Direction': str(direction)}})\n",
    "    for i in range(0, len(clusters)):\n",
    "        for node in clusters[i].nodes():\n",
    "            if node in anno_df.index:\n",
    "                pass\n",
    "            else:\n",
    "                nx.set_node_attributes(clusters[i], {node: {'Direction': 'NA'}})     \n",
    "                \n",
    "    # sort annotation for consistent display\n",
    "    for i in range(0, len(clusters)):\n",
    "        nodes_sorted2 = sorted(clusters[i].nodes(data=True), key = lambda x: (x[1]['source'], x[-1]['Direction']))\n",
    "        edges2 = sorted(clusters[i].edges(data=True))\n",
    "        clusters[i] = nx.Graph()\n",
    "        clusters[i].add_nodes_from(nodes_sorted2)\n",
    "        clusters[i].add_edges_from(edges2)\n",
    "        for node, d in clusters[i].nodes(data=True):\n",
    "            if node in anno_df.index:\n",
    "                pass\n",
    "            else:\n",
    "                d.pop('Direction', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142650ac",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3556e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello():\n",
    "    print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0caf1bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6a420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hello2():\n",
    "    print('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3aacd5",
   "metadata": {},
   "source": [
    "## Network Comparisons and Validation for cell specific signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a4eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare edge cell expr rank percentiles\n",
    "# networks: dictionary of cell specific network, key-indexed by cell type names - pre-selected for hyperparam\n",
    "def get_edge_scores_df(networks, int_df, mode='forest'):\n",
    "    rank_scores_df = pd.DataFrame(index = list(networks.keys()), columns = list(int_df.columns)[3:])\n",
    "    for key1 in rank_scores_df.index:\n",
    "        network = networks[key1]\n",
    "    \n",
    "        edge_df = nx.to_pandas_edgelist(network)\n",
    "        # if forest mode, then only take PCSF selected edges.\n",
    "        if mode == 'forest':\n",
    "            edge_df = edge_df[edge_df.in_solution == True]\n",
    "        if key1 == 'Naive':\n",
    "            df_result1 = pd.merge(edge_df.copy(), int_df, left_on = ['source','target'], right_on = ['protein1','protein2'])\n",
    "            df_result2= pd.merge(edge_df.copy(), int_df, left_on = ['source','target'], right_on = ['protein2','protein1'])\n",
    "            df_merged= df_result1.append(df_result2) # contains score info\n",
    "    \n",
    "        else:\n",
    "            df_merged = edge_df.copy()\n",
    "    \n",
    "        for key2 in rank_scores_df.columns:    \n",
    "            celltype = key2\n",
    "            rank_score = np.sum(df_merged[celltype])/len(df_merged)\n",
    "        \n",
    "            rank_scores_df.loc[key1, key2] = rank_score\n",
    "    rank_scores_df = rank_scores_df.astype('float')\n",
    "    return rank_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fcda835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare edge cell expr rank percentiles\n",
    "# networks: dictionary of cell specific network, key-indexed by cell type names - pre-selected for hyperparam\n",
    "# expr_df = adult_cortex_df or juv_cortex_df\n",
    "\n",
    "def get_node_scores_df(networks, int_df, expr_df, mode='Terminals'):\n",
    "    rank_scores_df = pd.DataFrame(index = list(networks.keys()), columns = list(int_df.columns)[3:])\n",
    "    for key1 in rank_scores_df.index:\n",
    "        network = networks[key1]\n",
    "    \n",
    "        node_df = oi.get_networkx_graph_as_dataframe_of_nodes(network)\n",
    "\n",
    "        # if forest mode, then only take PCSF selected edges.\n",
    "        if mode == 'Terminals':\n",
    "            node_df = node_df[node_df.terminal == True]\n",
    "        elif mode == 'Non-terminals':\n",
    "            node_df = node_df[node_df.terminal == False]\n",
    "        else:\n",
    "            print('proceeding with all nodes')\n",
    "            \n",
    "        inds = list(set(node_df.index.tolist()).intersection(set(expr_df.index.tolist())))\n",
    "    \n",
    "        for key2 in rank_scores_df.columns:    \n",
    "            celltype = key2 # i.e. Astrocyte_rank\n",
    "            gene_ranks = list(expr_df.loc[inds, celltype].values)\n",
    "        \n",
    "            rank_scores_df.loc[key1, key2] = sum(gene_ranks)/len(node_df)\n",
    "    rank_scores_df = rank_scores_df.astype('float')\n",
    "    return rank_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f253ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mode = 'forest' then only subset edges that are selected by PCSF\n",
    "# calculate pairwise jaccard index of networks\n",
    "def pairwise_jaccard_index_from_networks(networks, mode = 'augmented_forest'):\n",
    "    pairwise_df = pd.DataFrame(index = list(networks.keys()), columns = list(networks.keys()))\n",
    "    for key1 in pairwise_df.index:\n",
    "        for key2 in pairwise_df.index:\n",
    "            network_1 = networks[key1]\n",
    "            network_2 = networks[key2]\n",
    "            L1 = list(network_1.edges)\n",
    "            L2 = list(network_2.edges)\n",
    "            if mode == 'forest':\n",
    "                edge_df1 = nx.to_pandas_edgelist(network_1)\n",
    "                edge_df2 = nx.to_pandas_edgelist(network_2)\n",
    "                G1 = nx.from_pandas_edgelist(edge_df1[edge_df1.in_solution==True], source='source',target='target')\n",
    "                G2 = nx.from_pandas_edgelist(edge_df2[edge_df2.in_solution==True], source='source',target='target')\n",
    "                L1 = list(G1.edges)\n",
    "                L2 = list(G2.edges)\n",
    "            J = jaccard(L1, L2)\n",
    "            pairwise_df.loc[key1, key2] = J\n",
    "    pairwise_df = pairwise_df.astype('float')\n",
    "    return pairwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f9eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mode = 'forest' then only subset edges that are selected by PCSF\n",
    "# calculate pairwise jaccard index of networks\n",
    "def pairwise_jaccard_index_from_network_nodes(networks, mode = 'Terminal'):\n",
    "    \n",
    "    pairwise_df = pd.DataFrame(index = list(networks.keys()), columns = list(networks.keys()))\n",
    "    for key1 in pairwise_df.index:\n",
    "        for key2 in pairwise_df.index:\n",
    "            network_1 = networks[key1]\n",
    "            network_2 = networks[key2]\n",
    "            node_df1 = oi.get_networkx_graph_as_dataframe_of_nodes(network_1)\n",
    "            node_df2 = oi.get_networkx_graph_as_dataframe_of_nodes(network_2)\n",
    "            if mode == 'Terminal':\n",
    "                L1 = list(node_df1[node_df1['terminal'] == True].index)\n",
    "                L2 = list(node_df2[node_df2['terminal'] == True].index)\n",
    "            elif mode == 'Non-terminal':\n",
    "                L1 = list(node_df1[node_df1['terminal'] == False].index)\n",
    "                L2 = list(node_df2[node_df2['terminal'] == False].index)\n",
    "            elif mode == 'All':\n",
    "                L1 = list(node_df1.index)\n",
    "                L2 = list(node_df2.index)\n",
    "                            \n",
    "                \n",
    "            intersection = len(list(set(L1).intersection(set(L2))))\n",
    "            union = (len(L1) + len(L2)) - intersection\n",
    "                               \n",
    "            J = float(intersection) / union\n",
    "            pairwise_df.loc[key1, key2] = J\n",
    "    pairwise_df = pairwise_df.astype('float')\n",
    "    return pairwise_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2730dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
